<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>TF-VAEGAN</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://akshitac8.github.io/tfvaegan/img/classification_1.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://github.com/Anirudh257/strm"/>
    <meta property="og:title" content="TF-VAEGAN" />
    <meta property="og:description" content="Project page for Spatio-temporal Relation Modeling for Few-shot Action Recognition"/>

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="STRM" />
    <meta name="twitter:description" content="Project page for Spatio-temporal Relation Modeling for Few-shot Action Recognition" />
    <meta name="twitter:image" content="https://Anirudh257.github.io/images/strm_model.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Spatio-temporal Relation Modeling for Few-shot Action Recognition </br></b>
                <small>
                    <b>CVPR 2022</b>
                </small>https://mbzuai-cv-lab.netlify.app/author/dr.-rao-anwer/
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                     <li>
                        <a href=https://anirudh257.github.io//">
                            <b>Anirudh Thatipelli</b>
                        </a>
                        </br><b>MBZUAI</b>
                    </li>
                    <li>
                        <a href="https://sites.google.com/view/sanath-narayan/home">
                          <b>Sanath Narayan</b>
                        </a>
                        </br><b>IIAI</b>
                    </li>                                                          
                    <li>
                        <a href="https://salman-h-khan.github.io/">
                          <b>Salman Khan</b>
                        </a>
                        </br><b>MBZUAI, ANU</b>
                    </li>
                    <li>
                        <a href="https://mbzuai-cv-lab.netlify.app/author/dr.-rao-anwer/">
                          <b>Rao Mohammad Anwer</b>
                        </a>
                        </br><b>MBZUAI, Aalto University</b>
                    </li>                                         
                    <li>
                        <a href="https://sites.google.com/view/fahadkhans/home">
                            <b>Fahad Shahbaz Khan</b>
                        </a>
                        </br><b>MBZUAI, CVL, Linkoping University</b>
                    </li><br><br>
                    <li>
                        <a href="https://www.bernardghanem.com/">
                          <b>Bernard Ghanem</b>
                        </a>
                        </br><b>KAUST</b>
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2112.05132.pdf">
                            <image src="img/tfavegan_min_paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Anirudh257/strm">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Paperswithcode Badges</b>
                </h3>
                    <image src="img/papers_with_code_badge.png" class="img-responsive" alt="overview"><br>                <p class="text-justify">
                    Our proposed method TF-VAEGAN, current state-of-the-art for ZSL and GZSL (as seen from the above badges). Please do consider adding recent ZSL or GZSL results to the same.
                </p>
            </div>
        </div>

         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Video/Overview</b>
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/tNmyfKVUIpo" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Abstract</b>
                </h3>
                <p class="text-justify">
Zero-shot learning strives to classify unseen categories for which no data is available during training. In the generalized variant, the test samples can further belong to seen or unseen categories. The state-of-the-art relies on Generative Adversarial Networks that synthesize unseen class features by leveraging class-specific semantic embeddings. During training, they generate semantically consistent features, but discard this constraint during feature synthesis and classification. We propose to enforce semantic consistency at all stages of (generalized) zero-shot learning: training, feature synthesis and classification. We first introduce a feedback loop, from a semantic embedding decoder, that iteratively refines the generated features during both the training and feature synthesis stages. The synthesized features together with their corresponding latent embeddings from the decoder are then transformed into discriminative features and utilized during classification to reduce ambiguities among categories. Experiments on (generalized) zero-shot object and action classification reveal the benefit of semantic consistency and iterative feedback, outperforming existing methods on six zero-shot learning benchmarks.
                </p>
                <image src="img/paper_arch.png" class="img-responsive" alt="overview"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Classification Results</b>
                </h3>
                <p class="text-justify">
                   Below you will find quantitative results for ZSL and GZSL classification in comparison with the previous methods.
                </p>                
                <br>
                <image src="img/table_results.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                   Below you will find qualitative results for ZSL and GZSL classification in comparison with the previous methods. 
                   The top row shows different variations of the ground-truth class instances, the second and third rows show the classification predictions by the baseline and proposed approaches, respectively.
                   The <font style="color:green">green</font> and <font style="color:red">red</font> boxes denote correct and incorrect classification predictions, respectively.
                   The class names under each red box show the corresponding incorrectly predicted label. 
                </p>                
                <br>
                <image src="img/classification_1.png" class="img-responsive" alt="overview"><br>
                <image src="img/classification_2.png" class="img-responsive" alt="overview"><br>
                <h3>
                    <b>Image Reconstruction Results</b>
                </h3>
                <p class="text-justify">
                   Below you will find inverted images of Baseline synthesized features and our Feedback synthesized features on four example classes of oxford-flowers dataset.
                   These observations suggest that our Feedback improves the quality of synthesized features over the Baseline, where no feedback is present. 
                   <b>Best viewed in color and zoom.</b>
                </p>                
                <br>
                <image src="img/reconstruction.png" class="img-responsive center-block" alt="overview"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Citation</b>
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{thatipelli2021spatio,
    title={Spatio-temporal Relation Modeling for Few-shot Action Recognition},
    author={Thatipelli, Anirudh and Narayan, Sanath and Khan, Salman and Anwer, Rao Muhammad and Khan, Fahad Shahbaz and Ghanem, Bernard},
    booktitle={CVPR},
    year={2022}
}</textarea>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
