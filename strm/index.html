<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>STRM</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://github.com/Anirudh257/strm"/>
    <meta property="og:title" content="STRM" />
    <meta property="og:description" content="Project page for Spatio-temporal Relation Modeling for Few-shot Action Recognition"/>

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="STRM" />
    <meta name="twitter:description" content="Project page for Spatio-temporal Relation Modeling for Few-shot Action Recognition" />
    <meta name="twitter:image" content="https://Anirudh257.github.io/images/strm_model.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Spatio-temporal Relation Modeling for Few-shot Action Recognition </br></b>
                <small>
                    <b>CVPR 2022</b>
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                     <li>
                        <a href="https://anirudh257.github.io/">
                            <b>Anirudh Thatipelli</b>
                        </a>
                        </br><b>MBZUAI</b>
                    </li>
                    <li>
                        <a href="https://sites.google.com/view/sanath-narayan/home">
                          <b>Sanath Narayan</b>
                        </a>
                        </br><b>IIAI</b>
                    </li>                                                          
                    <li>
                        <a href="https://salman-h-khan.github.io/">
                          <b>Salman Khan</b>
                        </a>
                        </br><b>MBZUAI, ANU</b>
                    </li><br><br>
                    <li>
                        <a href="https://mbzuai-cv-lab.netlify.app/author/dr.-rao-anwer/">
                          <b>Rao Mohammad Anwer</b>
                        </a>
                        </br><b>MBZUAI, Aalto University</b>
                    </li>                                         
                    <li>
                        <a href="https://sites.google.com/view/fahadkhans/home">
                            <b>Fahad Shahbaz Khan</b>
                        </a>
                        </br><b>MBZUAI, Linkoping University</b>
                    </li>
                    <li>
                        <a href="https://www.bernardghanem.com/">
                          <b>Bernard Ghanem</b>
                        </a>
                        </br><b>KAUST</b>
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2112.05132.pdf">
                            <image src="images/strm.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Anirudh257/strm">
                            <image src="images/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Paperswithcode Badges</b>
                </h3>
                    <image src="images/papers_with_code_batch.png" class="img-responsive" alt="overview"><br>                <p class="text-justify">
                    Our proposed model, STRM attains current state-of-the-art for Few-shot Action Recognition. 
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Abstract</b>
                </h3>
                <p class="text-justify">
                   We propose a novel few-shot action recognition framework, STRM, which enhances class-specific feature discriminability while simultaneously learning higher-order temporal representations. The focus of our approach is a novel spatio-temporal enrichment module that aggregates spatial and temporal contexts with dedicated local patch-level and global frame-level feature enrichment sub-modules. Local patch-level enrichment captures the appearance-based characteristics of actions. On the other hand, global frame-level enrichment explicitly encodes the broad temporal context, thereby capturing the relevant object features over time. The resulting spatio-temporally enriched representations are then utilized to learn the relational matching between query and support action sub-sequences. We further introduce a query-class similarity classifier on the patch-level enriched features to enhance class-specific feature discriminability by reinforcing the feature learning at different stages in the proposed framework.Experiments are performed on four few-shot action recognition benchmarks: Kinetics, SSv2, HMDB51 and UCF101. Our extensive ablation study reveals the benefits of the proposed contributions. Furthermore, our approach sets a new state-of-the-art on all four benchmarks. On the challenging SSv2 benchmark, our approach achieves an absolute gain of <strong>3.5%</strong> in classification accuracy, as compared to the best existing method in the literature. 
                </p>
                <image src="images/strm_model.png" class="img-responsive" alt="overview"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>State-of-the-art comparison on four FS action recognition datasets.</b>
                </h3>
                <p class="text-justify">
                    Our STRM outperforms existing FS action recognition methods on all four datasets.                   
                </p>                
                <br>
                <image src="images/table_results.png" class="img-responsive center-block" alt="overview"><br>
                   </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Attention Visualization</b>
                </h3>
                <p class="text-justify">
                    Below you will find qualitative results with attention maps for few-shot action recognition. TRX struggles in case of spatial and 
                    temporal context variations that are commonly encountered in actions performed with different objects and backgrounds, 
                    e.g., fifth and sixth frame from the left in (b), where the regions corresponding to actions are not emphasized. Similarly, the action in the second and
                    third frame from the left in (d) is not accurately captured due to the distractor motion from the moving hand of another person. Our STRM approach 
                    explicitly enhances class-specific feature discriminability through spatio-temporal context aggregation and intermediate latent feature classification. 
                    This leads to better matching between query and limited support action instances.
                </p>                
                <br>
                <image src="https://ar5iv.labs.arxiv.org/html/2112.05132/assets/x1.png" class="img-responsive center-block" alt="overview"><br>
                    
                <image src ="https://ar5iv.labs.arxiv.org/html/2112.05132/assets/x5.png" class="img-responsive center-block" alt="overview"><br>
             </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Qualitative results </b>
                </h3>
                <p class="text-justify">
                    Below you will find quantitative comparisons between Baseline TRM and STRM w.r.t tuple matches. For the query tuple in 
                    <font style="color:green">green</font>, the best match obtained by our STRM (2nd support video) is a better representative, in 
                    comparison to the best match of Baseline TRM (1st support video).
                </p>                
                <br><image src="https://ar5iv.labs.arxiv.org/html/2112.05132/assets/x13.png" class="img-responsive center-block" alt="overview"><br>

                <p class="text-justify">
                 The Baseline TRM fails to obtain support tuples that are representative enough for the query tuples in <font style="color:red">red</font> 
                    and <font style="color:green">green</font>. Our STRM alleviates this issue and obtains good representative matches (and support videos) 
                    since it enhances the feature disriminability through patch-level as well as frame-level enrichment and learns higher-order temporal 
                    representations.
                </p>                
                <br><image src="https://ar5iv.labs.arxiv.org/html/2112.05132/assets/x13.png" class="img-responsive center-block" alt="overview"><br>
                    
                 
             </div>
        </div>
            
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Citation</b>
                </h3>
                <div class="form-group col-md-10 col-md-offset-1 text-indent: 50px">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{thatipelli2021spatio,
    title={Spatio-temporal Relation Modeling for Few-shot Action Recognition},
    author={Thatipelli, Anirudh and Narayan, Sanath and Khan, Salman and Anwer, Rao Muhammad and Khan, Fahad Shahbaz and Ghanem, Bernard},
    booktitle={CVPR},
    year={2022}
}</textarea>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
