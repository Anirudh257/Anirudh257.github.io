---
layout: default
title: Anirudh Thatipelli's blog
---
<div class="blurb">
	<h1>Hi there, I'm Anirudh Thatipelli!</h1>
	<link rel="stylesheet" type="text/css" href="stylesheet.css">
	
	  <link rel="icon" type="image/png" href="images/seal_icon.png">
	  <style>
	    #myimg{
	      width:100%;
	      max-width:100%;
	      border-radius:50%;
	      border: 1px solid #ddd;
	  padding: 5px;
	    }

	    p {
	      line-height: 22px;
	      font-size: 15px;
	    }

	    ul li{
	     font-size:15px;
	    }

	  </style>
	</head>

    <tr style="padding:0px">	
       <td style="padding:2.5%;width:63%;vertical-align:middle">

	<p>I am an MS CS student at UC Riverside. Previously, I worked as a research intern at <a href ="https://mbzuai-cv-lab.netlify.app/">Computer Vision Lab</a> at <b>MBZUAI</b>. I am fortunate to be 
	advised by <a href = "https://sites.google.com/view/sanath-narayan">Dr. Sanath Narayan</a> and	<a href = "https://sites.google.com/view/fahadkhans/home">Dr. Fahad Shahbaz Khan</a>. At MBZUAI, I am working on research projects dealing with Few-shot learning and
		Action Recognition. Before this, I also spent some fantastic time at <a href = "https://cvit.iiit.ac.in/">CVIT</a>, 
		<a href = "https://www.iiit.ac.in/"/>IIIT Hyderabad</a> where I worked under 
	<a href = "https://ravika.github.io/">Ravi Kiran Sarvadevabhatla</a> on the problem of skeleton action recognition. In summer of 2018, I have also 
	    worked on the topic of medical imaging under <a href = "https://www.iiit.ac.in/people/faculty/jsivaswamy/"/>Jayanthi Sivaswamy</a>. 

	I briefly worked for <a href = "https://www.dell.com/en-in?mn=SiORr6q_wPva1PZi_pK_p9-eYyuRWHmmL-cs.asXWybV9jyywKBux">Dell Internation Services</a> as a software development intern. 

        I am glad to be surrounded by excellent collaborators and mentors who helped me push beyond my boundaries.
	
	<p style="text-align:center">
	  <a href="mailto:thatipellianirudh@gmail.com">Email</a> &nbsp/&nbsp
	  <a href="https://scholar.google.com/citations?user=WVj4bQYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
	  <a href="https://twitter.com/AThatipelli">Twitter</a> &nbsp/&nbsp
	  <a href="https://github.com/Anirudh257">Github</a> &nbsp/&nbsp
	  <a href="https://in.linkedin.com/in/anirudh-thatipelli-85a650111/">Linkedin</a> &nbsp/&nbsp
	  <a href="resume/Anirudh_Thatipelli_resume.pdf">Resume/CV</a>
       </p>
	
	<p>I have started this blog to share and gain knowledge. I will be posting about my projects, interests and the stuff I do.</p>

	<p>I have added question papers, assignments of my courses at Shiv Nadar University.</p>

       </td>
    </tr>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p>
             	I am broadly interested in developing models that can learn from limited data and few training samples. Most of my research is developing 
		    such models for the task of Action Recognition. 
            </p>
          </td>
        </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/strm_model.png' width="160">
	      </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://anirudh257.github.io/strm">
		<papertitle>Spatio-temporal Relation Modeling for Few-shot Action Recognition</papertitle>
	      </a>
	      <br>
	      <strong>Anirudh Thatipelli</strong>,
	      <a href="https://sites.google.com/view/sanath-narayan">Sanath Narayan</a>,
	      <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
	      <a href = "https://scholar.google.fi/citations?user=_KlvMVoAAAAJ&hl=en">Rao Muhammad Anwer</a>,
	      <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan</a>,
	      <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>
	      <br>
	      <strong>CVPR 2022</strong>
	      <br>
	      <a href="https://arxiv.org/abs/2112.05132">paper</a> /
	      <a href="https://github.com/Anirudh257/strm">code</a>
	      <ul>
		<li>
		  <u>Description:</u> Proposed a novel spatio-temporal enrichment module, STRM for the problem of few-shot action recognition.
		</li>
		<li>
		  <u>Outcome:</u> Improved state-of-the-art performance on the challenging dataset of SSv2 by 3.5%. 
		</li>
	      </ul>
	    </td>
	  </tr>
     </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/quovadis-example.png' width="160">
	      </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://skeleton.iiit.ac.in/actionrecognition">
		<papertitle>Quo Vadis, Skeleton Action Recognition ?</papertitle>
	      </a>
	      <br>
	      <a href="https://pranaygupta36.github.io/">Pranay Guptan</a>,
	      <strong>Anirudh Thatipelli</strong>,
	      <a href="https://aditya2g.github.io/">Aditya Aggarwal</a>,
	      <a href = "https://in.linkedin.com/in/shubh-maheshwari-663737151">Shubh Maheshwari</a>,
	      <a href="https://researchweb.iiit.ac.in/~neel.trivedi/index.html">Neel Trivedi</a>,
	      <a href="https://fr.linkedin.com/in/sourav-das-087">Sourav Das</a>
	      <a href="https://ravika.github.io/">Ravi Kiran Sarvadevabhatla</a>
	      <br>
	      <strong>International Journal of Computer Vision (IJCV), Special Issue on Human pose, Motion, Activities and Shape in 3D, 2021</strong>
	      <br>
	      <a href="https://link.springer.com/article/10.1007/s11263-021-01470-y">paper</a> /
	      <a href="https://github.com/skelemoa/quovadis">code</a>
	      <ul>
		<li>
		  <u>Description:</u> In this work, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. We benchmark state-of-the-art models on the NTU-120 dataset and provide a multi-layered assessment.  
		</li>
		<li>
		  <u>Outcome:</u> We introduced Skeletics-152 and Skeleton-Mimetics datasets. Our results reveal the challenges and domain gap induced by actions 'in the wild' videos.
		</li>
	      </ul>
	    </td>
	  </tr>
     </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='https://github.com/skelemoa/ntu-x/raw/main/docs/images/NTU-X_sequence_diagram.png' width="160">
	      </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://skeleton.iiit.ac.in/ntux/">
		<papertitle>NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions</papertitle>
	      </a>
	      <br>
	      <a href="https://researchweb.iiit.ac.in/~neel.trivedi/index.html">Neel Trivedi</a>,
	      <strong>Anirudh Thatipelli</strong>,
	      <a href="https://ravika.github.io/">Ravi Kiran Sarvadevabhatla</a>
	      <br>
	      <strong>[ORAL] 12th Indian Conference on Computer Vision, Graphics and Image Processing(ICVGIP, 2021)</strong>
	      <br>
	      <a href="https://arxiv.org/abs/2101.11529v3">paper</a> /
	      <a href="https://github.com/skelemoa/ntu-x">code</a>
	      <ul>
		<li>
		  <u>Description:</u>In addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and facial joints, enabling a richer skeleton representation. We appropriately modify the state of the art approaches to enable training using the introduced datasets. Our results demonstrate the effectiveness of these NTU-X datasets in overcoming the aforementioned bottleneck and improve state of the art performance, overall and on previously worst performing action categories. 
		</li>
		<li>
		  <u>Outcome:</u> Our results demonstrate the effectiveness of these NTU-X datasets in overcoming the aforementioned bottleneck and improve state of the art performance, overall and on previously worst performing action categories.
		</li>
	      </ul>
	    </td>
	  </tr>
     </tbody></table>


	<p>Built this website using <a href ="https://jmcglone.com/guides/github-pages/">Guide</a> and <a target="_blank" href="https://jonbarron.info/">here</a></p>
</div><!-- /.blurb -->
